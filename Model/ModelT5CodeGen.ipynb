{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c68b8e7a-bc69-476c-874a-62d386544c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from google.cloud import storage\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12837efc-44d8-4d3a-bc37-51ee6a74fbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blob(bucket_name, filename):\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(filename)\n",
    "    return blob\n",
    "\n",
    "def get_blobs(bucket_name, folder):\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = list(bucket.list_blobs(prefix=folder))\n",
    "    return blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b027cc2-7159-491c-9333-18ed42105cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://data_tql/Model/2023-11-05/t5-small/run_5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_run_date = datetime.today().strftime('%Y-%m-%d')\n",
    "model_name = 't5-small'\n",
    "model_folder = f'gs://data_tql/Model/{model_run_date}/{model_name}'\n",
    "\n",
    "folder_items = get_blobs('data_tql', model_folder.replace(\"gs://data_tql/\",\"\"))\n",
    "\n",
    "pattern = r'run_(\\d+)'\n",
    "# Initialize an empty list to store the extracted run numbers\n",
    "run_numbers = []\n",
    "# Iterate through the folder paths and extract run numbers\n",
    "for folder_path in folder_items:\n",
    "    match = re.search(pattern, folder_path.name)\n",
    "    if match:\n",
    "        run_number = int(match.group(1))\n",
    "        run_numbers.append(run_number)\n",
    "\n",
    "# Find the maximum run number\n",
    "if run_numbers:\n",
    "    run_number = max(run_numbers) + 1\n",
    "else:\n",
    "    run_number = 1\n",
    "    \n",
    "model_folder = model_folder + '/run_' + f'{run_number}'\n",
    "model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89063740-992b-4fc1-990a-ca5e156bddc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "654d75cc-f2ef-4c61-b786-5c4b4f286515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>db_id</th>\n",
       "      <th>TQL</th>\n",
       "      <th>SQL</th>\n",
       "      <th>dataset</th>\n",
       "      <th>fileName</th>\n",
       "      <th>filePath</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>department_management</td>\n",
       "      <td>How many heads of the departments are older th...</td>\n",
       "      <td>SELECT count(*) FROM head WHERE age  &gt;  56</td>\n",
       "      <td>train</td>\n",
       "      <td>department_management.sqlite</td>\n",
       "      <td>sqliteDB/department_management.sqlite</td>\n",
       "      <td>{'count(*)': {0: 5}}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   db_id                                                TQL  \\\n",
       "0  department_management  How many heads of the departments are older th...   \n",
       "\n",
       "                                          SQL dataset  \\\n",
       "0  SELECT count(*) FROM head WHERE age  >  56   train   \n",
       "\n",
       "                       fileName                               filePath  \\\n",
       "0  department_management.sqlite  sqliteDB/department_management.sqlite   \n",
       "\n",
       "                 result  \n",
       "0  {'count(*)': {0: 5}}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>schema_id</th>\n",
       "      <th>table_name</th>\n",
       "      <th>table_name_original</th>\n",
       "      <th>primary_key</th>\n",
       "      <th>column_list</th>\n",
       "      <th>column_list_original</th>\n",
       "      <th>column_datatypes</th>\n",
       "      <th>foreign_keys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>perpetrator</td>\n",
       "      <td>perpetrator</td>\n",
       "      <td>perpetrator</td>\n",
       "      <td>Perpetrator_ID</td>\n",
       "      <td>['perpetrator id', 'people id', 'date', 'year'...</td>\n",
       "      <td>['Perpetrator_ID', 'People_ID', 'Date', 'Year'...</td>\n",
       "      <td>['number', 'number', 'text', 'number', 'text',...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>perpetrator</td>\n",
       "      <td>people</td>\n",
       "      <td>people</td>\n",
       "      <td>People_ID</td>\n",
       "      <td>['people id', 'name', 'height', 'weight', 'hom...</td>\n",
       "      <td>['People_ID', 'Name', 'Height', 'Weight', 'Hom...</td>\n",
       "      <td>['number', 'text', 'number', 'number', 'text']</td>\n",
       "      <td>[['perpetrator', 'People_ID', 'people', 'Peopl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     schema_id   table_name table_name_original     primary_key  \\\n",
       "0  perpetrator  perpetrator         perpetrator  Perpetrator_ID   \n",
       "1  perpetrator       people              people       People_ID   \n",
       "\n",
       "                                         column_list  \\\n",
       "0  ['perpetrator id', 'people id', 'date', 'year'...   \n",
       "1  ['people id', 'name', 'height', 'weight', 'hom...   \n",
       "\n",
       "                                column_list_original  \\\n",
       "0  ['Perpetrator_ID', 'People_ID', 'Date', 'Year'...   \n",
       "1  ['People_ID', 'Name', 'Height', 'Weight', 'Hom...   \n",
       "\n",
       "                                    column_datatypes  \\\n",
       "0  ['number', 'number', 'text', 'number', 'text',...   \n",
       "1     ['number', 'text', 'number', 'number', 'text']   \n",
       "\n",
       "                                        foreign_keys  \n",
       "0                                                 []  \n",
       "1  [['perpetrator', 'People_ID', 'people', 'Peopl...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "queryData = pd.read_csv('gs://data_tql/spider/processed/spiderQueryData.csv')\n",
    "tableData = pd.read_csv('gs://data_tql/spider/processed/Schemas/tablesSchemaSpider.csv')\n",
    "\n",
    "display(queryData.head(1))\n",
    "display(tableData.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "578140e7-0b9f-471b-a3c5-79fc1e849dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_schema_natural_language(row):\n",
    "\n",
    "    schema_id = row['schema_id']\n",
    "    table_name = row['table_name']\n",
    "    primary_key = row['primary_key']\n",
    "    column_list = eval(row['column_list_original'])\n",
    "    datatype_list = eval(row['column_datatypes'])\n",
    "    foreign_key = eval(row['foreign_keys'])\n",
    "\n",
    "    column_list_with_datatype = []\n",
    "    for column, datatype in zip(column_list, datatype_list):\n",
    "        column_list_with_datatype.append(' '.join([column, datatype]))\n",
    "        \n",
    "        \n",
    "    schema_natural_language = f\"CREATE TABLE {table_name} ({', '.join(column_list_with_datatype)}) which has {primary_key} as primary key\"\n",
    "\n",
    "    # schema_natural_language = f\"Given the table {table_name} having columns as {', '.join(column_list_with_datatype)} which has {primary_key} as primary key\"\n",
    "    return schema_natural_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eac0d407-fb7d-440e-8e46-e1b81508892e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('How many heads of the departments are older than 56 ? with tables: CREATE TABLE department (Department_ID number, Name text, Creation text, Ranking number, Budget_in_Billions number, Num_Employees number) which has Department_ID as primary key and CREATE TABLE head (head_ID number, name text, born_state text, age number) which has head_ID as primary key and CREATE TABLE management (department_ID number, head_ID number, temporary_acting text) which has department_ID as primary key',\n",
       " 'SELECT count(*) FROM head WHERE age  >  56')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tableData['schema_natural_language'] = tableData.apply(create_schema_natural_language, axis = 1)\n",
    "tableData.head(3)\n",
    "\n",
    "all_schemas = tableData['schema_id'].unique()\n",
    "schema_table_query = {}\n",
    "for schema in all_schemas:\n",
    "    schema_details = ' and '.join(tableData[tableData['schema_id'] == schema]['schema_natural_language'].values)\n",
    "    schema_table_query[schema] = schema_details\n",
    "\n",
    "queryData['schema_natural_language'] = queryData['db_id'].map(schema_table_query)\n",
    "queryData['final_TQL'] = queryData['TQL'] + ' with tables: ' + queryData['schema_natural_language']\n",
    "queryData.head(2)\n",
    "\n",
    "queryData['final_TQL'][0], queryData['SQL'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0378c89c-877a-4b7f-b4aa-d1c8551a386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained T5 model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = torch.nn.DataParallel(T5ForConditionalGeneration.from_pretrained('cssupport/t5-small-awesome-text-to-sql')).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c85462b-6251-4d03-965c-9828b7e7e095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset for training\n",
    "class SQLDataset(Dataset):\n",
    "    def __init__(self, input_texts, target_queries, tokenizer, task_prefix):\n",
    "        self.input_texts = input_texts\n",
    "        self.target_queries = target_queries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.task_prefix = task_prefix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        input_text = self.task_prefix + self.input_texts[index]\n",
    "        target_query = self.target_queries[index]\n",
    "\n",
    "        input_encoding = self.tokenizer([input_text], return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "        target_encoding = self.tokenizer([target_query], return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_encoding.input_ids.squeeze(0),\n",
    "            'attention_mask': input_encoding.attention_mask.squeeze(0),\n",
    "            'labels': target_encoding.input_ids.squeeze(0),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5997fc8-36f2-4f4a-8e29-3d88eabd0fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labeled dataset\n",
    "input_texts = queryData['final_TQL'].values # List of input texts\n",
    "target_queries = queryData['SQL'].values  # List of corresponding target SQL queries\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "train_input_texts, val_input_texts, train_target_queries, val_target_queries = train_test_split(input_texts, target_queries, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9e4108b-9d89-4171-9935-c8cbfc3ac827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the custom dataset\n",
    "task_prefix = 'SQL query for: '\n",
    "train_dataset = SQLDataset(train_input_texts, train_target_queries, tokenizer, task_prefix)\n",
    "val_dataset = SQLDataset(val_input_texts, val_target_queries, tokenizer, task_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53426756-3a09-4e73-9a1d-bd9c3f222be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.004\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Set up TensorBoard writer\n",
    "writer = SummaryWriter(f'{model_folder}/logs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61932c3c-7a49-4a6a-8e39-08cba4f42835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29be5392-4844-443f-9c2e-067047028c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de184928503415b82b07385bfde4d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Validation Loss: 0.0199\n",
      "Epoch: 2, Validation Loss: 0.0161\n",
      "Epoch: 3, Validation Loss: 0.0139\n",
      "Epoch: 4, Validation Loss: 0.0127\n",
      "Epoch: 5, Validation Loss: 0.0126\n",
      "Epoch: 6, Validation Loss: 0.0112\n",
      "Epoch: 7, Validation Loss: 0.0119\n",
      "Epoch: 8, Validation Loss: 0.0109\n",
      "Epoch: 9, Validation Loss: 0.0107\n",
      "Epoch: 10, Validation Loss: 0.0109\n",
      "Epoch: 11, Validation Loss: 0.0104\n",
      "Epoch: 12, Validation Loss: 0.0148\n",
      "Epoch: 13, Validation Loss: 0.0111\n",
      "Epoch: 14, Validation Loss: 0.0102\n",
      "Epoch: 15, Validation Loss: 0.0101\n",
      "Epoch: 16, Validation Loss: 0.0102\n",
      "Epoch: 17, Validation Loss: 0.0100\n",
      "Epoch: 18, Validation Loss: 0.0102\n",
      "Epoch: 19, Validation Loss: 0.0098\n",
      "Epoch: 20, Validation Loss: 0.0100\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device), labels=labels.to(device))\n",
    "        loss = outputs.loss\n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Write training loss to TensorBoard\n",
    "        writer.add_scalar('Training Loss', loss.mean().item(), epoch)\n",
    "\n",
    "    # Evaluation on validation set\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    for batch in val_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device), labels=labels.to(device))\n",
    "            val_loss = outputs.loss\n",
    "            total_val_loss += val_loss.mean().item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    \n",
    "    # Write validation loss to TensorBoard\n",
    "    writer.add_scalar('Validation Loss', avg_val_loss, epoch)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f'Epoch: {epoch+1}, Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "725ca50a-f4ee-4b8a-9825-182ece019ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file:///home/jupyter/model/events.out.tfevents.1699209479.jasmeet.49623.0 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/model/model.pt [Content-Type=application/vnd.snesdev-page-table]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "Copying file:///home/jupyter/model/events.out.tfevents.1698965850.jasmeet.120493.0 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/model/generation_config.json [Content-Type=application/json]...\n",
      "Copying file:///home/jupyter/model/pytorch_model.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/model/events.out.tfevents.1698954100.jasmeet.6221.0 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/model/events.out.tfevents.1699200885.jasmeet.4952.0 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/model/events.out.tfevents.1698771135.jasmeet.26921.0 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/model/config.json [Content-Type=application/json]...\n",
      "- [9/9 files][461.8 MiB/461.8 MiB] 100% Done                                    \n",
      "Operation completed over 9 objects/461.8 MiB.                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model upload successful.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import glob\n",
    "\n",
    "local_download_path = \"/home/jupyter/model\"\n",
    "model.module.save_pretrained(local_download_path)\n",
    "\n",
    "gsutil_command = f\"gsutil -m cp -r {local_download_path} {model_folder}\"\n",
    "\n",
    "# Run the gsutil command\n",
    "try:\n",
    "    subprocess.run(gsutil_command, shell=True, check=True, stdout=subprocess.PIPE)\n",
    "    print(\"Model upload successful.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Model upload failed. Error: {e}\")\n",
    "\n",
    "files = glob.glob(local_download_path+\"/*\")\n",
    "for f in files:\n",
    "    os.remove(f)    \n",
    "os.rmdir(local_download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0bad41c-c011-4523-bc7c-dee243fde28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://data_tql/Model/2023-11-05/t5-small/run_5'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "268f2fdc-3bce-4be8-b205-9b1701fba261",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jupyter/model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m      6\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(f)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_download_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jupyter/model'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "files = glob.glob(local_download_path+\"/*\")\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "os.rmdir(local_download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f20b026-2803-4901-88cc-e3a30291775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'{model_folder.replace(\"gs://data_tql/\",\"\")}/model.pt'\n",
    "blob_model = blob('data_tql', model_path)\n",
    "with blob_model.open(\"wb\", ignore_flush=True) as f:\n",
    "    torch.save(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b69614b-d0fb-4166-9532-c9d3426bc1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded Model/2023-11-05/t5-small/run_5/logs/events.out.tfevents.1699226419.jasmeet.4186.0 to /home/jupyter/model/events.out.tfevents.1699226419.jasmeet.4186.0\n",
      "Downloaded Model/2023-11-05/t5-small/run_5/model.pt to /home/jupyter/model/model.pt\n",
      "Downloaded Model/2023-11-05/t5-small/run_5/model/config.json to /home/jupyter/model/config.json\n",
      "Downloaded Model/2023-11-05/t5-small/run_5/model/events.out.tfevents.1698771135.jasmeet.26921.0 to /home/jupyter/model/events.out.tfevents.1698771135.jasmeet.26921.0\n",
      "Downloaded Model/2023-11-05/t5-small/run_5/model/events.out.tfevents.1698954100.jasmeet.6221.0 to /home/jupyter/model/events.out.tfevents.1698954100.jasmeet.6221.0\n",
      "Downloaded Model/2023-11-05/t5-small/run_5/model/events.out.tfevents.1698965850.jasmeet.120493.0 to /home/jupyter/model/events.out.tfevents.1698965850.jasmeet.120493.0\n",
      "Downloaded Model/2023-11-05/t5-small/run_5/model/events.out.tfevents.1699200885.jasmeet.4952.0 to /home/jupyter/model/events.out.tfevents.1699200885.jasmeet.4952.0\n",
      "Downloaded Model/2023-11-05/t5-small/run_5/model/events.out.tfevents.1699209479.jasmeet.49623.0 to /home/jupyter/model/events.out.tfevents.1699209479.jasmeet.49623.0\n",
      "Downloaded Model/2023-11-05/t5-small/run_5/model/generation_config.json to /home/jupyter/model/generation_config.json\n",
      "Downloaded Model/2023-11-05/t5-small/run_5/model/model.pt to /home/jupyter/model/model.pt\n",
      "Downloaded Model/2023-11-05/t5-small/run_5/model/pytorch_model.bin to /home/jupyter/model/pytorch_model.bin\n",
      "All files from gs://data_tql/Model/2023-11-05/t5-small/run_5 have been downloaded to /home/jupyter/model\n"
     ]
    }
   ],
   "source": [
    "folder_items = get_blobs('data_tql', model_folder.replace(\"gs://data_tql/\",\"\"))\n",
    "\n",
    "# Create the local directory if it doesn't exist\n",
    "os.makedirs(local_download_path, exist_ok=True)\n",
    "\n",
    "# Download each object in the folder\n",
    "for blob in folder_items:\n",
    "    # Create the local file path by combining the local directory with the object name\n",
    "    local_file_path = os.path.join(local_download_path, blob.name.split('/')[-1])\n",
    "\n",
    "    # Download the object to the local file path\n",
    "    blob.download_to_filename(local_file_path)\n",
    "\n",
    "    print(f'Downloaded {blob.name} to {local_file_path}')\n",
    "\n",
    "print(f'All files from {model_folder} have been downloaded to {local_download_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec0190c9-c57d-4755-9dd7-86e88a55ad88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Predicted Query:  SELECT DISTINCT COUNT ( DISTINCT t3.paperid ) FROM paperkeyphrase AS t1 JOIN keyphrase AS t4 ON t1.keyphraseid = t4.keyphraseid JOIN writes AS t3 ON t3.paperid = t1.paperid JOIN author AS t2 ON t3.authorid = t2.authorid WHERE t2.authorname = \"Ed Desmond\" AND t4.keyphrasename = \"Semantic Parsing\";\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Actual Query:  SELECT DISTINCT COUNT ( DISTINCT t3.paperid ) FROM paperkeyphrase AS t1 JOIN keyphrase AS t4 ON t1.keyphraseid  =  t4.keyphraseid JOIN writes AS t3 ON t3.paperid  =  t1.paperid JOIN author AS t2 ON t3.authorid  =  t2.authorid WHERE t2.authorname  =  \"Ed Desmond\" AND t4.keyphrasename  =  \"Semantic Parsing\";\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(local_download_path)\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# Move the model to GPU if available\n",
    "model.to(device)\n",
    "\n",
    "# Preprocess input text\n",
    "input_text = val_input_texts[1]\n",
    "sql = val_target_queries[1]\n",
    "\n",
    "# Tokenize and encode input text\n",
    "tokens = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=tokens.input_ids.to(device), max_new_tokens=1024)\n",
    "predicted_query = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# print(\"Input Text: \", input_text)\n",
    "print('-'*100)\n",
    "print(\"Predicted Query: \", predicted_query)\n",
    "print('-'*100)\n",
    "print(\"Actual Query: \", sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a63e0c5-f8aa-4a5d-a128-34ecbf96bba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01533cd7-3325-4583-8f86-08653ee2b45e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8228bae-9e40-4182-a5a1-da625f3164ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
